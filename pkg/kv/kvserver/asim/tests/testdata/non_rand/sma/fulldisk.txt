# This test verifies that the allocator correctly handles disk fullness by
#  shedding replicas from stores that exceed the storage capacity threshold. It
# sets up a cluster where one store (s5) has significantly less capacity (100 GiB)
# than others (512 GiB), causing it to hit the disk fullness threshold and
# continuously shed replicas to maintain disk usage less than 95%.
skip_under_ci
----

# Set every store's capacity to 512 GiB, we will later adjust just one store to
# have less free capacity.
gen_cluster nodes=5 store_byte_capacity_gib=512
----

gen_ranges ranges=500 bytes_mib=286
----

gen_load rate=500 max_block=60000 min_block=60000
----
29 MiB/s goodput

# Set the disk storage capacity of s5 to 100 GiB. This will necessitate
# shedding replicas from s5 continously as the workload fills up ranges.
set_capacity store=5 capacity=107374182400
----

# We will repeatedly hit the disk fullness threshold which causes shedding
# replicas on store 5. We should see s5 hovering right around 92.5-95%
# (the storage capacity threshold value).
eval duration=20m seed=42 metrics=(replicas,disk_fraction_used) cfgs=(sma-count,mma-only,mma-count)
----
disk_fraction_used#1: first: [s1=0.20, s2=0.20, s3=0.20, s4=0.20, s5=1.05] (stddev=0.34, mean=0.37, sum=2)
disk_fraction_used#1: last:  [s1=0.27, s2=0.27, s3=0.27, s4=0.27, s5=0.94] (stddev=0.27, mean=0.40, sum=2)
disk_fraction_used#1: thrash_pct: [s1=19%, s2=20%, s3=20%, s4=22%, s5=67%]  (sum=149%)
replicas#1: first: [s1=300, s2=300, s3=300, s4=300, s5=300] (stddev=0.00, mean=300.00, sum=1500)
replicas#1: last:  [s1=319, s2=317, s3=324, s4=323, s5=217] (stddev=41.58, mean=300.00, sum=1500)
replicas#1: thrash_pct: [s1=185%, s2=197%, s3=197%, s4=213%, s5=43%]  (sum=835%)
artifacts[sma-count]: 8473bd9f50baf595
==========================
disk_fraction_used#1: first: [s1=0.20, s2=0.20, s3=0.20, s4=0.20, s5=1.05] (stddev=0.34, mean=0.37, sum=2)
disk_fraction_used#1: last:  [s1=0.28, s2=0.28, s3=0.28, s4=0.26, s5=0.90] (stddev=0.25, mean=0.40, sum=2)
disk_fraction_used#1: thrash_pct: [s1=1%, s2=0%, s3=1%, s4=0%, s5=47%]  (sum=48%)
replicas#1: first: [s1=300, s2=300, s3=300, s4=300, s5=300] (stddev=0.00, mean=300.00, sum=1500)
replicas#1: last:  [s1=329, s2=330, s3=329, s4=305, s5=207] (stddev=47.45, mean=300.00, sum=1500)
replicas#1: thrash_pct: [s1=4%, s2=0%, s3=6%, s4=0%, s5=0%]  (sum=11%)
artifacts[mma-only]: 491e5ffd11727f20
==========================
disk_fraction_used#1: first: [s1=0.20, s2=0.20, s3=0.20, s4=0.20, s5=1.05] (stddev=0.34, mean=0.37, sum=2)
disk_fraction_used#1: last:  [s1=0.27, s2=0.27, s3=0.27, s4=0.27, s5=0.95] (stddev=0.27, mean=0.41, sum=2)
disk_fraction_used#1: thrash_pct: [s1=23%, s2=24%, s3=20%, s4=23%, s5=148%]  (sum=238%)
replicas#1: first: [s1=300, s2=300, s3=300, s4=300, s5=300] (stddev=0.00, mean=300.00, sum=1500)
replicas#1: last:  [s1=319, s2=320, s3=320, s4=322, s5=219] (stddev=40.51, mean=300.00, sum=1500)
replicas#1: thrash_pct: [s1=225%, s2=236%, s3=203%, s4=224%, s5=214%]  (sum=1102%)
artifacts[mma-count]: 76395c30fae521c
==========================
