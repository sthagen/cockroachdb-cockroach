skip_under_ci
----

gen_cluster nodes=9 node_cpu_rate_capacity=5000000000
----

# The placement will be skewed, s.t. n1/s1, n2/s2 and n3/s3 will have all the
# replicas initially and n1/s1 will have every lease. Each range is initially
# 256 MiB.
gen_ranges ranges=36 min_key=1 max_key=10000 placement_type=replica_placement bytes=268435456
{s1,s2,s3}:1
----
{s1:*,s2,s3}:1

# 5ms of request CPU per access and 500Âµs of raft CPU per write @ 1000/s.
gen_load rate=1000 rw_ratio=0.95 min_block=100 max_block=100 request_cpu_per_access=5000000 raft_cpu_per_write=500000 min_key=1 max_key=10000
----

# Almost empty workload, which generates no CPU and small amount of writes
# over the second half of the keyspace, scattered over s4-s9.
gen_ranges ranges=72 min_key=10001 max_key=20000 placement_type=replica_placement bytes=268435456
{s4,s5,s6}:1
{s7,s8,s9}:1
----
{s4:*,s5,s6}:1
{s7:*,s8,s9}:1

gen_load rate=100 rw_ratio=0 min_block=128 max_block=128 min_key=10001 max_key=20000
----

setting split_queue_enabled=false
----

# TODO(tbg): it's interesting that sma-only does better on write throughput than
# mma-only. Looking at the graphs, the mma-only flavor is much slower in moving
# load around. Possibly a bug?
eval duration=35m samples=1 seed=42 cfgs=(sma-count,mma-only,mma-count) metrics=(cpu,cpu_util,leases,replicas,write_bytes_per_second)
----
cpu#1: last:  [s1=984642922, s2=1126669982, s3=703798726, s4=419425213, s5=432156047, s6=559114683, s7=142318086, s8=283346689, s9=424209084] (stddev=303225920.55, mean=563964603.56, sum=5075681432)
cpu#1: thrash_pct: [s1=14%, s2=49%, s3=47%, s4=9%, s5=9%, s6=11%, s7=4%, s8=14%, s9=25%]  (sum=182%)
cpu_util#1: last:  [s1=0.20, s2=0.23, s3=0.14, s4=0.08, s5=0.09, s6=0.11, s7=0.03, s8=0.06, s9=0.08] (stddev=0.06, mean=0.11, sum=1)
cpu_util#1: thrash_pct: [s1=14%, s2=49%, s3=47%, s4=9%, s5=9%, s6=11%, s7=4%, s8=14%, s9=25%]  (sum=182%)
leases#1: first: [s1=36, s2=0, s3=0, s4=36, s5=0, s6=0, s7=36, s8=0, s9=0] (stddev=16.97, mean=12.00, sum=108)
leases#1: last:  [s1=13, s2=12, s3=12, s4=17, s5=11, s6=9, s7=15, s8=10, s9=9] (stddev=2.54, mean=12.00, sum=108)
leases#1: thrash_pct: [s1=53%, s2=38%, s3=44%, s4=21%, s5=20%, s6=26%, s7=15%, s8=26%, s9=37%]  (sum=281%)
replicas#1: first: [s1=36, s2=36, s3=36, s4=36, s5=36, s6=36, s7=36, s8=36, s9=36] (stddev=0.00, mean=36.00, sum=324)
replicas#1: last:  [s1=36, s2=37, s3=33, s4=37, s5=36, s6=36, s7=36, s8=35, s9=38] (stddev=1.33, mean=36.00, sum=324)
replicas#1: thrash_pct: [s1=433%, s2=402%, s3=672%, s4=135%, s5=117%, s6=217%, s7=117%, s8=218%, s9=403%]  (sum=2713%)
write_bytes_per_second#1: last:  [s1=5619, s2=5639, s3=5275, s4=6269, s5=6137, s6=6150, s7=6187, s8=5861, s9=6290] (stddev=336.10, mean=5936.33, sum=53427)
write_bytes_per_second#1: thrash_pct: [s1=572%, s2=656%, s3=832%, s4=513%, s5=540%, s6=593%, s7=564%, s8=563%, s9=701%]  (sum=5535%)
artifacts[sma-count]: b1d346c990864182
==========================
cpu#1: last:  [s1=571307704, s2=572893990, s3=576169245, s4=558125385, s5=555821700, s6=558003258, s7=570267358, s8=555474429, s9=556380794] (stddev=8080849.11, mean=563827095.89, sum=5074443863)
cpu#1: thrash_pct: [s1=10%, s2=77%, s3=62%, s4=10%, s5=23%, s6=10%, s7=11%, s8=12%, s9=18%]  (sum=234%)
cpu_util#1: last:  [s1=0.11, s2=0.11, s3=0.12, s4=0.11, s5=0.11, s6=0.11, s7=0.11, s8=0.11, s9=0.11] (stddev=0.00, mean=0.11, sum=1)
cpu_util#1: thrash_pct: [s1=10%, s2=77%, s3=62%, s4=10%, s5=23%, s6=10%, s7=11%, s8=12%, s9=18%]  (sum=234%)
leases#1: first: [s1=36, s2=0, s3=0, s4=36, s5=0, s6=0, s7=36, s8=0, s9=0] (stddev=16.97, mean=12.00, sum=108)
leases#1: last:  [s1=4, s2=4, s3=4, s4=40, s5=4, s6=4, s7=40, s8=4, s9=4] (stddev=14.97, mean=12.00, sum=108)
leases#1: thrash_pct: [s1=0%, s2=48%, s3=37%, s4=0%, s5=11%, s6=0%, s7=0%, s8=0%, s9=6%]  (sum=102%)
replicas#1: first: [s1=36, s2=36, s3=36, s4=36, s5=36, s6=36, s7=36, s8=36, s9=36] (stddev=0.00, mean=36.00, sum=324)
replicas#1: last:  [s1=27, s2=28, s3=29, s4=40, s5=40, s6=40, s7=40, s8=40, s9=40] (stddev=5.68, mean=36.00, sum=324)
replicas#1: thrash_pct: [s1=0%, s2=0%, s3=0%, s4=0%, s5=30%, s6=0%, s7=0%, s8=0%, s9=16%]  (sum=46%)
write_bytes_per_second#1: last:  [s1=3756, s2=3912, s3=4018, s4=6894, s5=6901, s6=6901, s7=7022, s8=7008, s9=7004] (stddev=1444.42, mean=5935.11, sum=53416)
write_bytes_per_second#1: thrash_pct: [s1=573%, s2=577%, s3=606%, s4=147%, s5=174%, s6=146%, s7=173%, s8=170%, s9=185%]  (sum=2752%)
artifacts[mma-only]: e97065eb8e46fba6
==========================
cpu#1: last:  [s1=571403856, s2=571966513, s3=574554118, s4=557196019, s5=557400913, s6=556275032, s7=554695440, s8=558495278, s9=571805040] (stddev=7863014.08, mean=563754689.89, sum=5073792209)
cpu#1: thrash_pct: [s1=74%, s2=114%, s3=106%, s4=20%, s5=50%, s6=12%, s7=42%, s8=76%, s9=36%]  (sum=530%)
cpu_util#1: last:  [s1=0.11, s2=0.11, s3=0.11, s4=0.11, s5=0.11, s6=0.11, s7=0.11, s8=0.11, s9=0.11] (stddev=0.00, mean=0.11, sum=1)
cpu_util#1: thrash_pct: [s1=74%, s2=114%, s3=106%, s4=20%, s5=50%, s6=12%, s7=42%, s8=76%, s9=36%]  (sum=530%)
leases#1: first: [s1=36, s2=0, s3=0, s4=36, s5=0, s6=0, s7=36, s8=0, s9=0] (stddev=16.97, mean=12.00, sum=108)
leases#1: last:  [s1=7, s2=7, s3=7, s4=15, s5=14, s6=14, s7=15, s8=14, s9=15] (stddev=3.56, mean=12.00, sum=108)
leases#1: thrash_pct: [s1=66%, s2=89%, s3=77%, s4=34%, s5=39%, s6=8%, s7=52%, s8=62%, s9=21%]  (sum=448%)
replicas#1: first: [s1=36, s2=36, s3=36, s4=36, s5=36, s6=36, s7=36, s8=36, s9=36] (stddev=0.00, mean=36.00, sum=324)
replicas#1: last:  [s1=36, s2=35, s3=35, s4=36, s5=38, s6=35, s7=38, s8=35, s9=36] (stddev=1.15, mean=36.00, sum=324)
replicas#1: thrash_pct: [s1=257%, s2=317%, s3=317%, s4=143%, s5=177%, s6=117%, s7=119%, s8=288%, s9=171%]  (sum=1906%)
write_bytes_per_second#1: last:  [s1=5314, s2=5327, s3=5247, s4=6158, s5=6519, s6=6033, s7=6519, s8=6075, s9=6218] (stddev=479.56, mean=5934.44, sum=53410)
write_bytes_per_second#1: thrash_pct: [s1=1127%, s2=977%, s3=1107%, s4=452%, s5=571%, s6=475%, s7=596%, s8=734%, s9=610%]  (sum=6651%)
artifacts[mma-count]: 6162a7d88999c81
==========================
